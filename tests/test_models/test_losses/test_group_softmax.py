"""Script:
python tests/test_models/test_losses/test_group_softmax.py
"""

import jittor as jt
from jdet.models.losses import GroupSoftmax

N = 8
num_classes = 10
start = 0
num_logits = 2

labels = jt.Var([1,0,3,2,4])
print("labels:\n", labels)

# test narrow
cls_score = jt.Var(
   [[0.47881365, 0.420919  , 0.38578004, 0.8052165,  0.27370733],
    [0.19900735, 0.7165177 , 0.5128623,  0.6768294,  0.4716686 ],
    [0.33413434, 0.79596156, 0.7257531,  0.7326412,  0.5006631 ],
    [0.64515287, 0.08451196, 0.3924798,  0.40834078, 0.9835877 ],
    [0.1577293, 0.95628715, 0.3183534,  0.5656762,  0.3205039 ]]
)
pred = cls_score[:, start:start+num_logits, ...]
print("pred:\n", pred)

# test nonzero
haha = jt.Var([1, 1, 1, 0, 1])
print("jt.nonzero(haha):", jt.nonzero(haha))
print("jt.nonzero(haha).shape:", jt.nonzero(haha).shape)
print("jt.nonzero(haha).reshape(-1):", jt.nonzero(haha).reshape(-1))
print("jt.nonzero(haha).reshape(-1).shape[0]:",
      jt.nonzero(haha).reshape(-1).shape[0])

loss_cls = GroupSoftmax()
print("loss_cls.group_ids:", loss_cls.group_ids)
print("loss_cls.n_cls_group:", loss_cls.n_cls_group)
print("loss_cls.get_channel_num(num_classes):",
      loss_cls.get_channel_num(num_classes))

# cls_score = jt.rand((N, loss_cls.get_channel_num(num_classes)))
cls_score = jt.Var([
    [0.79050220, 0.97076950, 0.72356370, 0.93417996, 0.76225543, 0.22735824, 0.20982690, 0.56065000, 0.84434890, 0.97257185, 0.01553369, 0.07472585, 0.91741663, 0.02161808, 0.33499792],
    [0.31029204, 0.07845172, 0.53807545, 0.43362070, 0.86315775, 0.09184323, 0.60912347, 0.53809120, 0.69866070, 0.39035320, 0.66607934, 0.79584104, 0.70058614, 0.75150570, 0.55628127],
    [0.41971612, 0.16862471, 0.07550719, 0.04935989, 0.59172136, 0.06116220, 0.95314770, 0.55377800, 0.34640718, 0.06561315, 0.76023823, 0.32416055, 0.16615154, 0.50893220, 0.62351720],
    [0.45424858, 0.55605450, 0.60789860, 0.95198520, 0.01454274, 0.41990685, 0.37427425, 0.42737950, 0.96690970, 0.85115670, 0.39053730, 0.76003100, 0.84040093, 0.61897760, 0.15625821],
    [0.23164402, 0.24109207, 0.03448509, 0.59083843, 0.22170085, 0.12611629, 0.63644380, 0.71101910, 0.09826450, 0.53142935, 0.73302550, 0.95946825, 0.78327150, 0.44391727, 0.91765124],
    [0.96422200, 0.67961514, 0.29179570, 0.21025935, 0.82882696, 0.09436616, 0.01197731, 0.30274808, 0.28723133, 0.49665746, 0.32171490, 0.06252977, 0.93787160, 0.80766827, 0.48039454],
    [0.99120003, 0.09885466, 0.45030364, 0.25343335, 0.45423730, 0.36636677, 0.52652740, 0.34579110, 0.71129530, 0.74037370, 0.46047520, 0.20701629, 0.32276696, 0.74416840, 0.23881836],
    [0.82017887, 0.74579114, 0.51188385, 0.23201536, 0.48199543, 0.89750624, 0.38686320, 0.00989924, 0.37657350, 0.07080555, 0.02888087, 0.40078962, 0.07137158, 0.54210660, 0.18610023]
])
print("cls_score:", cls_score)
# label = jt.randint(low=0, high=num_classes, shape=(N,), dtype='int32')
label = jt.Var([7, 8, 4, 3, 2, 2, 8, 1])
print("label:", label)
print("loss_cls(cls_score, label):", loss_cls(cls_score, label)) # 4.6707