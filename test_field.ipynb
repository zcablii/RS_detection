{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train and val dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 4000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "data_dir = '/media/data3/lyx/Detection/data/train/images/'\n",
    "files = sorted(glob.glob(data_dir+ \"/*.*\"))\n",
    "file_ids = []\n",
    "for each in files:\n",
    "    file_ids.append(each.split('/')[-1][:-4])\n",
    "\n",
    "data_num = len(file_ids)\n",
    "\n",
    "total_ind = range(0, data_num)\n",
    "val_ind = sorted(random.sample(range(0, data_num), data_num//5))\n",
    "\n",
    "train_ind = [x for x in total_ind if x not in val_ind]\n",
    "\n",
    "val_data = [file_ids[i] for i in val_ind]\n",
    "train_data = [file_ids[i] for i in train_ind]\n",
    "f = open('train_val.txt','w')\n",
    "f.write(str(train_data))\n",
    "f.write(str(val_data))\n",
    "f.close()\n",
    "\n",
    "label_dir = '/media/data3/lyx/Detection/data/train/labelXml/'\n",
    "val_data_img = [data_dir+i+'.tif' for i in val_data]\n",
    "train_data_img = [data_dir+i+'.tif' for i in train_data]\n",
    "val_data_label = [label_dir+i+'.xml' for i in val_data]\n",
    "train_data_label = [label_dir+i+'.xml' for i in train_data]\n",
    "\n",
    "\n",
    "val_img_dst = '/media/data3/lyx/Detection/data_train_val/val/image/'\n",
    "val_label_dst = '/media/data3/lyx/Detection/data_train_val/val/label/'\n",
    "train_img_dst = '/media/data3/lyx/Detection/data_train_val/train/image/'\n",
    "train_label_dst = '/media/data3/lyx/Detection/data_train_val/train/label/'\n",
    "for path in (val_img_dst,val_label_dst,train_img_dst,train_label_dst):\n",
    "    os.makedirs(path) \n",
    "val_data_img_dst = [val_img_dst+i+'.tif' for i in val_data]\n",
    "train_data_img_dst = [train_img_dst+i+'.tif' for i in train_data]\n",
    "val_data_label_dst = [val_label_dst+i+'.xml' for i in val_data]\n",
    "train_data_label_dst = [train_label_dst+i+'.xml' for i in train_data]\n",
    "import shutil\n",
    "\n",
    "\n",
    "pair = zip([val_data_img,train_data_img,val_data_label,train_data_label],[val_data_img_dst,train_data_img_dst,val_data_label_dst,train_data_label_dst])\n",
    "for src,dst in pair:\n",
    "    for src,dst in zip(src,dst):\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jittor version kld, gwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jt.Var([0.3865626], dtype=float32)\n",
      "jt.Var([0.81335044], dtype=float32)\n",
      "jt.Var([0.43135446], dtype=float32)\n",
      "jt.Var([0.7188362], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import jittor as jt\n",
    "\n",
    "from jittor.nn import bmm as bmm\n",
    "from copy import deepcopy\n",
    "\n",
    "from jittor import nn \n",
    "\n",
    "def xy_wh_r_2_xy_sigma(xywhr):\n",
    "    \"\"\"Convert oriented bounding box to 2-D Gaussian distribution.\n",
    "\n",
    "    Args:\n",
    "        xywhr (torch.Tensor): rbboxes with shape (N, 5).\n",
    "\n",
    "    Returns:\n",
    "        xy (torch.Tensor): center point of 2-D Gaussian distribution\n",
    "            with shape (N, 2).\n",
    "        sigma (torch.Tensor): covariance matrix of 2-D Gaussian distribution\n",
    "            with shape (N, 2, 2).\n",
    "    \"\"\"\n",
    "    _shape = xywhr.shape\n",
    "    assert _shape[-1] == 5\n",
    "    xy = xywhr[..., :2]\n",
    "    wh = xywhr[..., 2:4].clamp(min_v=1e-7, max_v=1e7).reshape(-1, 2)\n",
    "    r = xywhr[..., 4]\n",
    "    cos_r = jt.cos(r)\n",
    "    sin_r = jt.sin(r)\n",
    "    R = jt.stack((cos_r, -sin_r, sin_r, cos_r), dim=-1).reshape(-1, 2, 2)\n",
    "    S = 0.5 * jt.stack([jt.misc.diag(wh[i], diagonal=0) for i in range(wh.shape[0])]).reshape(-1, 2, 2)\n",
    "    sigma = bmm(bmm(R,S*S),R.permute(0, 2,\n",
    "                                            1)).reshape(_shape[:-1] + (2, 2))\n",
    "\n",
    "    return xy, sigma\n",
    "\n",
    "\n",
    "def postprocess(distance, fun='log1p', tau=1.0):\n",
    "\n",
    "    if fun == 'log1p':\n",
    "        distance = jt.log(distance+1)\n",
    "    elif fun == 'sqrt':\n",
    "        distance = jt.sqrt(distance.clamp(min_v=1e-7))\n",
    "    elif fun == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Invalid non-linear function {fun}')\n",
    "\n",
    "    if tau >= 1.0:\n",
    "        return 1 - 1 / (tau + distance)\n",
    "    else:\n",
    "        return distance\n",
    "\n",
    "\n",
    "def kld_loss_v0(pred, target, fun='log1p', tau=1.0, alpha=1.0, sqrt=True, weight=None, reduction='mean', avg_factor=None):\n",
    "\n",
    "    xy_p, Sigma_p = pred\n",
    "    xy_t, Sigma_t = target\n",
    "    _shape = xy_p.shape\n",
    "\n",
    "    xy_p = xy_p.reshape(-1, 2)\n",
    "    xy_t = xy_t.reshape(-1, 2)\n",
    "    Sigma_p = Sigma_p.reshape(-1, 2, 2)\n",
    "    Sigma_t = Sigma_t.reshape(-1, 2, 2)\n",
    "    Sigma_p_inv = jt.stack((Sigma_p[..., 1, 1], -Sigma_p[..., 0, 1],\n",
    "                               -Sigma_p[..., 1, 0], Sigma_p[..., 0, 0]),\n",
    "                              dim=-1).reshape(-1, 2, 2) \n",
    "    \n",
    "    Sigma_p_inv = Sigma_p_inv / jt.linalg.det(Sigma_p).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    dxy = (xy_p - xy_t).unsqueeze(-1)\n",
    "    xy_distance = 0.5 * bmm(bmm(dxy.permute(0, 2, 1),(Sigma_p_inv)), dxy).view(-1)\n",
    "\n",
    "    whr_distance = 0.5 * bmm(Sigma_p_inv,Sigma_t)\n",
    "    whr_distance = jt.stack([jt.misc.diag(whr_distance[i]) for i in range(whr_distance.shape[0])]).sum(dim=-1)\n",
    "\n",
    "\n",
    "    Sigma_p_det_log = jt.linalg.det(Sigma_p).log()\n",
    "    Sigma_t_det_log = jt.linalg.det(Sigma_t).log()\n",
    "\n",
    "    whr_distance = whr_distance + 0.5 * (Sigma_p_det_log - Sigma_t_det_log)\n",
    "    whr_distance = whr_distance - 1\n",
    "    distance = (xy_distance / (alpha * alpha) + whr_distance)\n",
    "    if sqrt:\n",
    "        distance = distance.clamp(min_v=1e-7).sqrt()\n",
    "\n",
    "    distance = distance.reshape(_shape[:-1])\n",
    "\n",
    "    loss = postprocess(distance, fun=fun, tau=tau)\n",
    "  \n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kld_loss(pred, target, fun='log1p', tau=1.0, weight=None, reduction='mean', avg_factor=None):\n",
    "    xy_p, Sigma_p = pred\n",
    "    xy_t, Sigma_t = target\n",
    "\n",
    "    xy_p = xy_p.reshape(-1, 2)\n",
    "    xy_t = xy_t.reshape(-1, 2)\n",
    "    Sigma_p = Sigma_p.reshape(-1, 2, 2)\n",
    "    Sigma_t = Sigma_t.reshape(-1, 2, 2)\n",
    "\n",
    "    \n",
    "    delta = (xy_p - xy_t).unsqueeze(-1)\n",
    "    sigma_t_inv = jt.linalg.inv(Sigma_t)\n",
    "    term1 = delta.transpose(-1,\n",
    "                            -2).matmul(sigma_t_inv).matmul(delta).squeeze(-1)\n",
    "    x_ = sigma_t_inv.matmul(Sigma_p)\n",
    "    term2_ = jt.stack([jt.misc.diag(x_[i]) for i in range(x_.shape[0])]).sum(dim=-1).reshape(-1,1)\n",
    "    term2 = term2_ + jt.log(jt.linalg.det(Sigma_t) / jt.linalg.det(Sigma_p)).reshape(-1, 1)\n",
    "    \n",
    "    dis = term1 + term2 - 2\n",
    "    kl_dis = dis.clamp(min_v=1e-6)\n",
    "\n",
    "    if fun == 'sqrt':\n",
    "        loss = 1 - 1 / (tau + jt.sqrt(kl_dis))\n",
    "    else:\n",
    "        loss = 1 - 1 / (tau + jt.log(kl_dis+1))\n",
    "    # return kl_loss\n",
    " \n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        loss = loss.sum()/avg_factor\n",
    "        # print('jt.grad(loss, Sigma_p)', jt.grad(loss, Sigma_p))\n",
    "        return loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gwd_loss_v0(pred, target, fun='log1p', tau=1.0, alpha=1.0, normalize=True, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Gaussian Wasserstein distance loss.\n",
    "\n",
    "    \"\"\"\n",
    "    xy_p, Sigma_p = pred\n",
    "    xy_t, Sigma_t = target\n",
    "\n",
    "    xy_distance = ((xy_p - xy_t)*(xy_p - xy_t)).sum(dim=-1)\n",
    "    \n",
    "    whr_distance = jt.stack([jt.misc.diag(Sigma_p[i]) for i in range(Sigma_p.shape[0])]).sum(dim=-1)\n",
    "    whr_distance = whr_distance + jt.stack([jt.misc.diag(Sigma_t[i]) for i in range(Sigma_t.shape[0])]).sum(dim=-1)\n",
    "    x_ = bmm(Sigma_p,Sigma_t)\n",
    "    _t_tr =jt.stack([jt.misc.diag(x_[i]) for i in range(x_.shape[0])]).sum(dim=-1)\n",
    "    _t_det_sqrt = (jt.linalg.det(Sigma_p) * jt.linalg.det(Sigma_t)).clamp(1e-7).sqrt()\n",
    "    whr_distance = whr_distance + (-2) * (\n",
    "        (_t_tr + 2 * _t_det_sqrt).clamp(1e-7).sqrt())\n",
    "    distance = (xy_distance + alpha * alpha * whr_distance).clamp(1e-7).sqrt()\n",
    "    if normalize:\n",
    "        scale = 2 * (\n",
    "            _t_det_sqrt.clamp(1e-7).sqrt().clamp(1e-7).sqrt()).clamp(1e-7)\n",
    "        distance = distance / scale\n",
    "\n",
    "    \n",
    "    loss = postprocess(distance, fun=fun, tau=tau)\n",
    "\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "    return loss\n",
    "\n",
    "def gwd_loss(pred, target, fun='log1p', tau=2.0, normalize=True, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Gaussian Wasserstein distance loss.\n",
    "    \"\"\"\n",
    "    xy_p, Sigma_p = pred\n",
    "    xy_t, Sigma_t = target\n",
    "\n",
    "    xy_distance = ((xy_p - xy_t)*(xy_p - xy_t)).sum(dim=-1)\n",
    "\n",
    "    whr_distance = jt.stack([jt.misc.diag(Sigma_p[i]) for i in range(Sigma_p.shape[0])]).sum(dim=-1)\n",
    "    whr_distance = whr_distance + jt.stack([jt.misc.diag(Sigma_t[i]) for i in range(Sigma_t.shape[0])]).sum(dim=-1)\n",
    "    x_ = bmm(Sigma_p,Sigma_t)\n",
    "    _t_tr =jt.stack([jt.misc.diag(x_[i]) for i in range(x_.shape[0])]).sum(dim=-1)\n",
    "    _t_det_sqrt = (jt.linalg.det(Sigma_p) * jt.linalg.det(Sigma_t)).clamp(0).sqrt()\n",
    "    whr_distance = whr_distance + (-2) * (\n",
    "        (_t_tr + 2 * _t_det_sqrt).clamp(0).sqrt())\n",
    "    distance = xy_distance + whr_distance\n",
    "    gwd_dis = distance.clamp(min_v=1e-6)\n",
    "    if fun == 'sqrt':\n",
    "        loss = 1 - 1 / (tau + jt.sqrt(gwd_dis))\n",
    "    elif fun == 'log1p':\n",
    "        loss = 1 - 1 / (tau + jt.log(gwd_dis+1))\n",
    "    else:\n",
    "        scale = 2 * (_t_det_sqrt.sqrt().sqrt()).clamp(1e-7)\n",
    "        loss = jt.log1p(jt.sqrt(gwd_dis) / scale)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class GDLoss(nn.Module):\n",
    "\n",
    "    BAG_GD_LOSS = {\n",
    "        'gwd_v0': gwd_loss_v0,\n",
    "        'kld_v0': kld_loss_v0,\n",
    "        'gwd': gwd_loss,\n",
    "        'kld': kld_loss,\n",
    "    }\n",
    "    BAG_PREP = {\n",
    "        # 'xy_stddev_pearson': xy_stddev_pearson_2_xy_sigma,\n",
    "        'xy_wh_r': xy_wh_r_2_xy_sigma\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 loss_type,\n",
    "                 representation='xy_wh_r',\n",
    "                 fun='log1p',\n",
    "                 tau=1.0,\n",
    "                 alpha=1.0,\n",
    "                 reduction='mean',\n",
    "                 loss_weight=1.0,\n",
    "                 **kwargs):\n",
    "        super(GDLoss, self).__init__()\n",
    "        assert reduction in ['none', 'sum', 'mean']\n",
    "        assert fun in ['log1p', 'none', 'sqrt']\n",
    "        assert loss_type in self.BAG_GD_LOSS\n",
    "        self.loss = self.BAG_GD_LOSS[loss_type]\n",
    "        self.preprocess = self.BAG_PREP[representation]\n",
    "        self.fun = fun\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def execute(self,\n",
    "                pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None,\n",
    "                **kwargs):\n",
    "\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "        if (weight is not None) and (not jt.any(weight > 0)) and (\n",
    "                reduction != 'none'):\n",
    "            return (pred * weight).sum()\n",
    "        if weight is not None and len(weight.shape) > 1:\n",
    "            assert weight.shape == pred.shape\n",
    "            weight = weight.mean(-1)\n",
    "        _kwargs = deepcopy(self.kwargs)\n",
    "        _kwargs.update(kwargs)\n",
    "        pred_ = self.preprocess(pred)\n",
    "    \n",
    "        target_ = self.preprocess(target)\n",
    "        loss = self.loss(\n",
    "            pred_,\n",
    "            target_,\n",
    "            fun=self.fun,\n",
    "            tau=self.tau,\n",
    "            weight=weight,\n",
    "            avg_factor=avg_factor,\n",
    "            reduction=reduction,\n",
    "            **_kwargs) * self.loss_weight\n",
    "        # print('loss',loss)\n",
    "        return loss\n",
    "\n",
    "Loss = GDLoss('gwd_v0')\n",
    "\n",
    "preds = jt.Var([[10.,15.,10.,20.,0.5],[10.,15.,10.,20.,0.5]])\n",
    "target = jt.Var([[15.,15.,10.,5.,0.1],[15.,15.,10.,5.,0.1]])\n",
    "\n",
    "Loss_gwd_v0 = GDLoss('gwd_v0')\n",
    "Loss_gwd = GDLoss('gwd')\n",
    "Loss_kld_v0 = GDLoss('kld_v0')\n",
    "Loss_kld = GDLoss('kld')\n",
    "L = (Loss_gwd_v0,Loss_gwd,Loss_kld_v0,Loss_kld )\n",
    "for each in L:\n",
    "    print(each(preds,target ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch version kld, gwd (From https://github.com/open-mmlab/mmrotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3866)\n",
      "tensor(0.8134)\n",
      "tensor(0.4314)\n",
      "tensor(0.7188)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def xy_wh_r_2_xy_sigma(xywhr):\n",
    "    \"\"\"Convert oriented bounding box to 2-D Gaussian distribution.\n",
    "\n",
    "    Args:\n",
    "        xywhr (torch.Tensor): rbboxes with shape (N, 5).\n",
    "\n",
    "    Returns:\n",
    "        xy (torch.Tensor): center point of 2-D Gaussian distribution\n",
    "            with shape (N, 2).\n",
    "        sigma (torch.Tensor): covariance matrix of 2-D Gaussian distribution\n",
    "            with shape (N, 2, 2).\n",
    "    \"\"\"\n",
    "    _shape = xywhr.shape\n",
    "    assert _shape[-1] == 5\n",
    "    xy = xywhr[..., :2]\n",
    "    wh = xywhr[..., 2:4].clamp(min=1e-7, max=1e7).reshape(-1, 2)\n",
    "    r = xywhr[..., 4]\n",
    "    cos_r = torch.cos(r)\n",
    "    sin_r = torch.sin(r)\n",
    "    R = torch.stack((cos_r, -sin_r, sin_r, cos_r), dim=-1).reshape(-1, 2, 2)\n",
    "    S = 0.5 * torch.diag_embed(wh)\n",
    "\n",
    "    sigma = R.bmm(S.square()).bmm(R.permute(0, 2,\n",
    "                                            1)).reshape(_shape[:-1] + (2, 2))\n",
    "\n",
    "    return xy, sigma\n",
    "\n",
    "def postprocess(distance, fun='log1p', tau=1.0):\n",
    "    \"\"\"Convert distance to loss.\n",
    "\n",
    "    Args:\n",
    "        distance (torch.Tensor)\n",
    "        fun (str, optional): The function applied to distance.\n",
    "            Defaults to 'log1p'.\n",
    "        tau (float, optional): Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        loss (torch.Tensor)\n",
    "    \"\"\"\n",
    "    if fun == 'log1p':\n",
    "        distance = torch.log1p(distance)\n",
    "    elif fun == 'sqrt':\n",
    "        distance = torch.sqrt(distance.clamp(1e-7))\n",
    "    elif fun == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Invalid non-linear function {fun}')\n",
    "\n",
    "    if tau >= 1.0:\n",
    "        return 1 - 1 / (tau + distance)\n",
    "    else:\n",
    "        return distance\n",
    "\n",
    "def gwd_loss_v0(pred, target, fun='log1p', tau=1.0, alpha=1.0, normalize=True,weight=None, reduction='mean', avg_factor=None):\n",
    "\n",
    "    xy_p, Sigma_p = pred\n",
    "    xy_t, Sigma_t = target\n",
    "\n",
    "    xy_distance = (xy_p - xy_t).square().sum(dim=-1)\n",
    "\n",
    "    whr_distance = Sigma_p.diagonal(dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    whr_distance = whr_distance + Sigma_t.diagonal(\n",
    "        dim1=-2, dim2=-1).sum(dim=-1)\n",
    "\n",
    "    _t_tr = (Sigma_p.bmm(Sigma_t)).diagonal(dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    _t_det_sqrt = (Sigma_p.det() * Sigma_t.det()).clamp(1e-7).sqrt()\n",
    "    whr_distance = whr_distance + (-2) * (\n",
    "        (_t_tr + 2 * _t_det_sqrt).clamp(1e-7).sqrt())\n",
    "\n",
    "    distance = (xy_distance + alpha * alpha * whr_distance).clamp(1e-7).sqrt()\n",
    "\n",
    "    if normalize:\n",
    "        scale = 2 * (\n",
    "            _t_det_sqrt.clamp(1e-7).sqrt().clamp(1e-7).sqrt()).clamp(1e-7)\n",
    "        distance = distance / scale\n",
    "    \n",
    "    loss = postprocess(distance, fun=fun, tau=tau)\n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def kld_loss_v0(pred, target, fun='log1p', tau=1.0, alpha=1.0, sqrt=True,weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Kullback-Leibler Divergence loss.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted bboxes.\n",
    "        target (torch.Tensor): Corresponding gt bboxes.\n",
    "        fun (str): The function applied to distance. Defaults to 'log1p'.\n",
    "        tau (float): Defaults to 1.0.\n",
    "        alpha (float): Defaults to 1.0.\n",
    "        sqrt (bool): Whether to sqrt the distance. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        loss (torch.Tensor)\n",
    "    \"\"\"\n",
    "    xy_p, Sigma_p = pred\n",
    "    xy_t, Sigma_t = target\n",
    "\n",
    "    _shape = xy_p.shape\n",
    "\n",
    "    xy_p = xy_p.reshape(-1, 2)\n",
    "    xy_t = xy_t.reshape(-1, 2)\n",
    "    Sigma_p = Sigma_p.reshape(-1, 2, 2)\n",
    "    Sigma_t = Sigma_t.reshape(-1, 2, 2)\n",
    "\n",
    "    Sigma_p_inv = torch.stack((Sigma_p[..., 1, 1], -Sigma_p[..., 0, 1],\n",
    "                               -Sigma_p[..., 1, 0], Sigma_p[..., 0, 0]),\n",
    "                              dim=-1).reshape(-1, 2, 2)\n",
    "    Sigma_p_inv = Sigma_p_inv / Sigma_p.det().unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    dxy = (xy_p - xy_t).unsqueeze(-1)\n",
    "    xy_distance = 0.5 * dxy.permute(0, 2, 1).bmm(Sigma_p_inv).bmm(dxy).view(-1)\n",
    "\n",
    "    whr_distance = 0.5 * Sigma_p_inv.bmm(Sigma_t).diagonal(\n",
    "        dim1=-2, dim2=-1).sum(dim=-1)\n",
    "\n",
    "    Sigma_p_det_log = Sigma_p.det().log()\n",
    "    Sigma_t_det_log = Sigma_t.det().log()\n",
    "    whr_distance = whr_distance + 0.5 * (Sigma_p_det_log - Sigma_t_det_log)\n",
    "    whr_distance = whr_distance - 1\n",
    "    distance = (xy_distance / (alpha * alpha) + whr_distance)\n",
    "    if sqrt:\n",
    "        distance = distance.clamp(1e-7).sqrt()\n",
    "\n",
    "    distance = distance.reshape(_shape[:-1])\n",
    "\n",
    "    loss = postprocess(distance, fun=fun, tau=tau)\n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def gwd_loss(pred, target, fun='log1p', tau=2.0,weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Gaussian Wasserstein distance loss.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted bboxes.\n",
    "        target (torch.Tensor): Corresponding gt bboxes.\n",
    "        fun (str): The function applied to distance. Defaults to 'log1p'.\n",
    "        tau (float): Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        loss (torch.Tensor)\n",
    "    \"\"\"\n",
    "    mu_p, sigma_p = pred\n",
    "    mu_t, sigma_t = target\n",
    "\n",
    "    xy_distance = (mu_p - mu_t).square().sum(dim=-1)\n",
    "\n",
    "    whr_distance = sigma_p.diagonal(dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    whr_distance = whr_distance + sigma_t.diagonal(\n",
    "        dim1=-2, dim2=-1).sum(dim=-1)\n",
    "\n",
    "    _t_tr = (sigma_p.bmm(sigma_t)).diagonal(dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    _t_det_sqrt = (sigma_p.det() * sigma_t.det()).clamp(0).sqrt()\n",
    "    whr_distance += (-2) * (_t_tr + 2 * _t_det_sqrt).clamp(0).sqrt()\n",
    "\n",
    "    dis = xy_distance + whr_distance\n",
    "    gwd_dis = dis.clamp(min=1e-6)\n",
    "\n",
    "    if fun == 'sqrt':\n",
    "        loss = 1 - 1 / (tau + torch.sqrt(gwd_dis))\n",
    "    elif fun == 'log1p':\n",
    "        loss = 1 - 1 / (tau + torch.log1p(gwd_dis))\n",
    "    else:\n",
    "        scale = 2 * (_t_det_sqrt.sqrt().sqrt()).clamp(1e-7)\n",
    "        loss = torch.log1p(torch.sqrt(gwd_dis) / scale)\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def kld_loss(pred, target, fun='log1p', tau=1.0,weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Kullback-Leibler Divergence loss.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted bboxes.\n",
    "        target (torch.Tensor): Corresponding gt bboxes.\n",
    "        fun (str): The function applied to distance. Defaults to 'log1p'.\n",
    "        tau (float): Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        loss (torch.Tensor)\n",
    "    \"\"\"\n",
    "    mu_p, sigma_p = pred\n",
    "    mu_t, sigma_t = target\n",
    "\n",
    "    mu_p = mu_p.reshape(-1, 2)\n",
    "    mu_t = mu_t.reshape(-1, 2)\n",
    "    sigma_p = sigma_p.reshape(-1, 2, 2)\n",
    "    sigma_t = sigma_t.reshape(-1, 2, 2)\n",
    "\n",
    "    delta = (mu_p - mu_t).unsqueeze(-1)\n",
    "    sigma_t_inv = torch.inverse(sigma_t)\n",
    "    term1 = delta.transpose(-1,\n",
    "                            -2).matmul(sigma_t_inv).matmul(delta).squeeze(-1)\n",
    "    term2 = torch.diagonal(\n",
    "        sigma_t_inv.matmul(sigma_p),\n",
    "        dim1=-2, dim2=-1).sum(dim=-1, keepdim=True) + \\\n",
    "        torch.log(torch.det(sigma_t) / torch.det(sigma_p)).reshape(-1, 1)\n",
    "    dis = term1 + term2 - 2\n",
    "    kl_dis = dis.clamp(min=1e-6)\n",
    "\n",
    "    if fun == 'sqrt':\n",
    "        loss = 1 - 1 / (tau + torch.sqrt(kl_dis))\n",
    "    else:\n",
    "        loss = 1 - 1 / (tau + torch.log1p(kl_dis))\n",
    "    \n",
    "    if weight is not None:\n",
    "        loss *= weight\n",
    "    \n",
    "    if avg_factor is None:\n",
    "        avg_factor = loss.numel()\n",
    "    \n",
    "    if reduction==\"sum\":\n",
    "        return loss.sum() \n",
    "    elif reduction == \"mean\":\n",
    "        return loss.sum()/avg_factor\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class GDLoss(nn.Module):\n",
    "    \"\"\"Gaussian based loss.\n",
    "\n",
    "    Args:\n",
    "        loss_type (str):  Type of loss.\n",
    "        representation (str, optional): Coordinate System.\n",
    "        fun (str, optional): The function applied to distance.\n",
    "            Defaults to 'log1p'.\n",
    "        tau (float, optional): Defaults to 1.0.\n",
    "        alpha (float, optional): Defaults to 1.0.\n",
    "        reduction (str, optional): The reduction method of the\n",
    "            loss. Defaults to 'mean'.\n",
    "        loss_weight (float, optional): The weight of loss. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        loss (torch.Tensor)\n",
    "    \"\"\"\n",
    "    BAG_GD_LOSS = {\n",
    "        'gwd': gwd_loss,\n",
    "        'kld': kld_loss,\n",
    "        'gwd_v0': gwd_loss_v0,\n",
    "        'kld_v0': kld_loss_v0,\n",
    "    }\n",
    "    BAG_PREP = {\n",
    "        'xy_wh_r': xy_wh_r_2_xy_sigma\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 loss_type,\n",
    "                 representation='xy_wh_r',\n",
    "                 fun='log1p',\n",
    "                 tau=1.0,\n",
    "                 reduction='mean',\n",
    "                 loss_weight=1.0,\n",
    "                 **kwargs):\n",
    "        super(GDLoss, self).__init__()\n",
    "        assert reduction in ['none', 'sum', 'mean']\n",
    "        assert fun in ['log1p', 'none', 'sqrt']\n",
    "        assert loss_type in self.BAG_GD_LOSS\n",
    "        self.loss = self.BAG_GD_LOSS[loss_type]\n",
    "        self.preprocess = self.BAG_PREP[representation]\n",
    "        self.fun = fun\n",
    "        self.tau = tau\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self,\n",
    "                pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None,\n",
    "                **kwargs):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted convexes.\n",
    "            target (torch.Tensor): Corresponding gt convexes.\n",
    "            weight (torch.Tensor, optional): The weight of loss for each\n",
    "                prediction. Defaults to None.\n",
    "            avg_factor (int, optional): Average factor that is used to average\n",
    "                the loss. Defaults to None.\n",
    "            reduction_override (str, optional): The reduction method used to\n",
    "               override the original reduction method of the loss.\n",
    "               Defaults to None.\n",
    "        \"\"\"\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "        if (weight is not None) and (not torch.any(weight > 0)) and (\n",
    "                reduction != 'none'):\n",
    "            return (pred * weight).sum()\n",
    "        if weight is not None and weight.dim() > 1:\n",
    "            assert weight.shape == pred.shape\n",
    "            weight = weight.mean(-1)\n",
    "        _kwargs = deepcopy(self.kwargs)\n",
    "        _kwargs.update(kwargs)\n",
    "\n",
    "        pred = self.preprocess(pred)\n",
    "        target = self.preprocess(target)\n",
    "\n",
    "        return self.loss(\n",
    "            pred,\n",
    "            target,\n",
    "            fun=self.fun,\n",
    "            tau=self.tau,\n",
    "            weight=weight,\n",
    "            avg_factor=avg_factor,\n",
    "            reduction=reduction,\n",
    "            **_kwargs) * self.loss_weight\n",
    "\n",
    "\n",
    "preds = torch.Tensor([[10.,15.,10.,20.,0.5],[10.,15.,10.,20.,0.5]])\n",
    "target = torch.Tensor([[15.,15.,10.,5.,0.1],[15.,15.,10.,5.,0.1]])\n",
    "Loss_gwd_v0 = GDLoss('gwd_v0')\n",
    "Loss_gwd = GDLoss('gwd')\n",
    "Loss_kld_v0 = GDLoss('kld_v0')\n",
    "Loss_kld = GDLoss('kld')\n",
    "L = (Loss_gwd_v0,Loss_gwd,Loss_kld_v0,Loss_kld )\n",
    "for each in L:\n",
    "    print(each(preds,target ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.6352],\n",
       "        [ 2.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[[ 2.1162066e+00, -2.2864749e+00],\n",
    "  [-5.5145769e+00,  1.3518972e+01]],\n",
    "\n",
    " [[ 1.0000001e+00, -7.0014124e-08],\n",
    "  [ 9.3700834e-09,  1.0000000e+00]]])\n",
    "\n",
    "torch.diagonal( a,\n",
    "        dim1=-2, dim2=-1).sum(dim=-1, keepdim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jittor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "866ebf4ae3c95aeda7f8f2d5ed834225867c604b4a8f9bce3a4957565c899f04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
